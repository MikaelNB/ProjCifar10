{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classList = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
    "batch_size = 100\n",
    "num_classes = 10\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16, (3, 3), padding='same', input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using data augmentation.\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "50000/50000 [==============================] - 78s - loss: 3.5202 - acc: 0.1364 - val_loss: 2.1411 - val_acc: 0.2047\n",
      "Epoch 2/100\n",
      "50000/50000 [==============================] - 81s - loss: 2.1587 - acc: 0.1773 - val_loss: 1.9684 - val_acc: 0.2729\n",
      "Epoch 3/100\n",
      "50000/50000 [==============================] - 81s - loss: 2.0324 - acc: 0.2359 - val_loss: 1.7914 - val_acc: 0.3445\n",
      "Epoch 4/100\n",
      "50000/50000 [==============================] - 82s - loss: 1.9179 - acc: 0.2818 - val_loss: 1.6888 - val_acc: 0.3965\n",
      "Epoch 5/100\n",
      "50000/50000 [==============================] - 82s - loss: 1.8214 - acc: 0.3264 - val_loss: 1.5822 - val_acc: 0.4360\n",
      "Epoch 6/100\n",
      "50000/50000 [==============================] - 82s - loss: 1.7323 - acc: 0.3637 - val_loss: 1.4927 - val_acc: 0.4623\n",
      "Epoch 7/100\n",
      "50000/50000 [==============================] - 81s - loss: 1.6640 - acc: 0.3905 - val_loss: 1.4386 - val_acc: 0.4917\n",
      "Epoch 8/100\n",
      "50000/50000 [==============================] - 80s - loss: 1.6058 - acc: 0.4126 - val_loss: 1.4402 - val_acc: 0.4945\n",
      "Epoch 9/100\n",
      "50000/50000 [==============================] - 80s - loss: 1.5537 - acc: 0.4334 - val_loss: 1.3651 - val_acc: 0.5214\n",
      "Epoch 10/100\n",
      "50000/50000 [==============================] - 80s - loss: 1.5144 - acc: 0.4499 - val_loss: 1.3111 - val_acc: 0.5376\n",
      "Epoch 11/100\n",
      "50000/50000 [==============================] - 80s - loss: 1.4708 - acc: 0.4669 - val_loss: 1.2874 - val_acc: 0.5438\n",
      "Epoch 12/100\n",
      "50000/50000 [==============================] - 80s - loss: 1.4267 - acc: 0.4867 - val_loss: 1.2527 - val_acc: 0.5606\n",
      "Epoch 13/100\n",
      "50000/50000 [==============================] - 80s - loss: 1.3984 - acc: 0.4968 - val_loss: 1.2555 - val_acc: 0.5530\n",
      "Epoch 14/100\n",
      "50000/50000 [==============================] - 80s - loss: 1.3650 - acc: 0.5109 - val_loss: 1.1949 - val_acc: 0.5758\n",
      "Epoch 15/100\n",
      "50000/50000 [==============================] - 80s - loss: 1.3330 - acc: 0.5237 - val_loss: 1.1750 - val_acc: 0.5874\n",
      "Epoch 16/100\n",
      "50000/50000 [==============================] - 80s - loss: 1.3034 - acc: 0.5338 - val_loss: 1.1657 - val_acc: 0.5878\n",
      "Epoch 17/100\n",
      "50000/50000 [==============================] - 80s - loss: 1.2769 - acc: 0.5461 - val_loss: 1.1839 - val_acc: 0.5865\n",
      "Epoch 18/100\n",
      "50000/50000 [==============================] - 80s - loss: 1.2494 - acc: 0.5563 - val_loss: 1.1559 - val_acc: 0.5981\n",
      "Epoch 19/100\n",
      "50000/50000 [==============================] - 80s - loss: 1.2309 - acc: 0.5630 - val_loss: 1.1141 - val_acc: 0.6108\n",
      "Epoch 20/100\n",
      "50000/50000 [==============================] - 80s - loss: 1.2034 - acc: 0.5710 - val_loss: 1.0922 - val_acc: 0.6208\n",
      "Epoch 21/100\n",
      "50000/50000 [==============================] - 80s - loss: 1.1829 - acc: 0.5801 - val_loss: 1.0949 - val_acc: 0.6209\n",
      "Epoch 22/100\n",
      "50000/50000 [==============================] - 79s - loss: 1.1651 - acc: 0.5880 - val_loss: 1.0905 - val_acc: 0.6217\n",
      "Epoch 23/100\n",
      "50000/50000 [==============================] - 80s - loss: 1.1456 - acc: 0.5953 - val_loss: 1.0632 - val_acc: 0.6296\n",
      "Epoch 24/100\n",
      "50000/50000 [==============================] - 80s - loss: 1.1308 - acc: 0.6005 - val_loss: 1.0530 - val_acc: 0.6398\n",
      "Epoch 25/100\n",
      "50000/50000 [==============================] - 80s - loss: 1.1177 - acc: 0.6061 - val_loss: 1.0645 - val_acc: 0.6384\n",
      "Epoch 26/100\n",
      "50000/50000 [==============================] - 80s - loss: 1.1019 - acc: 0.6105 - val_loss: 1.0470 - val_acc: 0.6432\n",
      "Epoch 27/100\n",
      "50000/50000 [==============================] - 79s - loss: 1.0840 - acc: 0.6190 - val_loss: 1.0337 - val_acc: 0.6453\n",
      "Epoch 28/100\n",
      "50000/50000 [==============================] - 78s - loss: 1.0658 - acc: 0.6272 - val_loss: 1.0407 - val_acc: 0.6362\n",
      "Epoch 29/100\n",
      "50000/50000 [==============================] - 79s - loss: 1.0598 - acc: 0.6285 - val_loss: 1.0316 - val_acc: 0.6464\n",
      "Epoch 30/100\n",
      "50000/50000 [==============================] - 80s - loss: 1.0478 - acc: 0.6355 - val_loss: 1.0108 - val_acc: 0.6560\n",
      "Epoch 31/100\n",
      "50000/50000 [==============================] - 80s - loss: 1.0371 - acc: 0.6359 - val_loss: 1.0363 - val_acc: 0.6407\n",
      "Epoch 32/100\n",
      "50000/50000 [==============================] - 83s - loss: 1.0276 - acc: 0.6436 - val_loss: 1.0163 - val_acc: 0.6569\n",
      "Epoch 33/100\n",
      "50000/50000 [==============================] - 82s - loss: 1.0152 - acc: 0.6457 - val_loss: 1.0140 - val_acc: 0.6557\n",
      "Epoch 34/100\n",
      "50000/50000 [==============================] - 80s - loss: 1.0033 - acc: 0.6512 - val_loss: 1.0071 - val_acc: 0.6569\n",
      "Epoch 35/100\n",
      "50000/50000 [==============================] - 81s - loss: 0.9966 - acc: 0.6537 - val_loss: 1.0225 - val_acc: 0.6524\n",
      "Epoch 36/100\n",
      "50000/50000 [==============================] - 81s - loss: 0.9844 - acc: 0.6565 - val_loss: 1.0231 - val_acc: 0.6548\n",
      "Epoch 37/100\n",
      "50000/50000 [==============================] - 79s - loss: 0.9815 - acc: 0.6594 - val_loss: 0.9856 - val_acc: 0.6636\n",
      "Epoch 38/100\n",
      "50000/50000 [==============================] - 79s - loss: 0.9688 - acc: 0.6614 - val_loss: 0.9852 - val_acc: 0.6632\n",
      "Epoch 39/100\n",
      "50000/50000 [==============================] - 79s - loss: 0.9627 - acc: 0.6645 - val_loss: 0.9783 - val_acc: 0.6683\n",
      "Epoch 40/100\n",
      "50000/50000 [==============================] - 80s - loss: 0.9539 - acc: 0.6701 - val_loss: 0.9693 - val_acc: 0.6701\n",
      "Epoch 41/100\n",
      "50000/50000 [==============================] - 80s - loss: 0.9441 - acc: 0.6722 - val_loss: 0.9816 - val_acc: 0.6666\n",
      "Epoch 42/100\n",
      "50000/50000 [==============================] - 80s - loss: 0.9377 - acc: 0.6718 - val_loss: 1.0003 - val_acc: 0.6708\n",
      "Epoch 43/100\n",
      "50000/50000 [==============================] - 82s - loss: 0.9359 - acc: 0.6768 - val_loss: 0.9979 - val_acc: 0.6629\n",
      "Epoch 44/100\n",
      "50000/50000 [==============================] - 82s - loss: 0.9245 - acc: 0.6766 - val_loss: 1.0071 - val_acc: 0.6677\n",
      "Epoch 45/100\n",
      "50000/50000 [==============================] - 82s - loss: 0.9219 - acc: 0.6815 - val_loss: 0.9973 - val_acc: 0.6724\n",
      "Epoch 46/100\n",
      "50000/50000 [==============================] - 80s - loss: 0.9180 - acc: 0.6835 - val_loss: 0.9903 - val_acc: 0.6727\n",
      "Epoch 47/100\n",
      "50000/50000 [==============================] - 84s - loss: 0.9164 - acc: 0.6829 - val_loss: 0.9816 - val_acc: 0.6763\n",
      "Epoch 48/100\n",
      "50000/50000 [==============================] - 85s - loss: 0.9066 - acc: 0.6867 - val_loss: 1.0036 - val_acc: 0.6653\n",
      "Epoch 49/100\n",
      "50000/50000 [==============================] - 86s - loss: 0.9066 - acc: 0.6860 - val_loss: 1.0473 - val_acc: 0.6681\n",
      "Epoch 50/100\n",
      "50000/50000 [==============================] - 81s - loss: 0.9003 - acc: 0.6904 - val_loss: 0.9860 - val_acc: 0.6693\n",
      "Epoch 51/100\n",
      "50000/50000 [==============================] - 82s - loss: 0.8889 - acc: 0.6917 - val_loss: 0.9859 - val_acc: 0.6784\n",
      "Epoch 52/100\n",
      "50000/50000 [==============================] - 80s - loss: 0.8923 - acc: 0.6914 - val_loss: 1.0030 - val_acc: 0.6806\n",
      "Epoch 53/100\n",
      "50000/50000 [==============================] - 80s - loss: 0.8862 - acc: 0.6944 - val_loss: 1.0375 - val_acc: 0.6688\n",
      "Epoch 54/100\n",
      "50000/50000 [==============================] - 78s - loss: 0.8826 - acc: 0.6953 - val_loss: 1.0422 - val_acc: 0.6649\n",
      "Epoch 55/100\n",
      "50000/50000 [==============================] - 83s - loss: 0.8836 - acc: 0.6952 - val_loss: 1.0123 - val_acc: 0.6784\n",
      "Epoch 56/100\n",
      "50000/50000 [==============================] - 80s - loss: 0.8782 - acc: 0.6955 - val_loss: 0.9948 - val_acc: 0.6759\n",
      "Epoch 57/100\n",
      "50000/50000 [==============================] - 81s - loss: 0.8703 - acc: 0.6967 - val_loss: 1.0376 - val_acc: 0.6643\n",
      "Epoch 58/100\n",
      "50000/50000 [==============================] - 79s - loss: 0.8676 - acc: 0.6981 - val_loss: 1.1578 - val_acc: 0.6724\n",
      "Epoch 59/100\n",
      "50000/50000 [==============================] - 79s - loss: 0.8631 - acc: 0.7016 - val_loss: 0.9987 - val_acc: 0.6708\n",
      "Epoch 60/100\n",
      "50000/50000 [==============================] - 80s - loss: 0.8667 - acc: 0.7023 - val_loss: 1.0331 - val_acc: 0.6854\n",
      "Epoch 61/100\n",
      "50000/50000 [==============================] - 80s - loss: 0.8624 - acc: 0.7024 - val_loss: 0.9811 - val_acc: 0.6845\n",
      "Epoch 62/100\n",
      "50000/50000 [==============================] - 82s - loss: 0.8614 - acc: 0.7042 - val_loss: 1.0190 - val_acc: 0.6759\n",
      "Epoch 63/100\n",
      "50000/50000 [==============================] - 81s - loss: 0.8574 - acc: 0.7028 - val_loss: 1.0427 - val_acc: 0.6670\n",
      "Epoch 64/100\n",
      "50000/50000 [==============================] - 80s - loss: 0.8507 - acc: 0.7063 - val_loss: 1.1403 - val_acc: 0.6680\n",
      "Epoch 65/100\n",
      "50000/50000 [==============================] - 85s - loss: 0.8479 - acc: 0.7064 - val_loss: 1.0225 - val_acc: 0.6811\n",
      "Epoch 66/100\n",
      "50000/50000 [==============================] - 81s - loss: 0.8441 - acc: 0.7091 - val_loss: 1.0711 - val_acc: 0.6738\n",
      "Epoch 67/100\n",
      "50000/50000 [==============================] - 81s - loss: 0.8539 - acc: 0.7071 - val_loss: 1.0010 - val_acc: 0.6813\n",
      "Epoch 68/100\n",
      "50000/50000 [==============================] - 82s - loss: 0.8408 - acc: 0.7092 - val_loss: 1.0008 - val_acc: 0.6782\n",
      "Epoch 69/100\n",
      "50000/50000 [==============================] - 81s - loss: 0.8450 - acc: 0.7070 - val_loss: 1.0193 - val_acc: 0.6723\n",
      "Epoch 70/100\n",
      "50000/50000 [==============================] - 78s - loss: 0.8482 - acc: 0.7085 - val_loss: 1.1557 - val_acc: 0.6847\n",
      "Epoch 71/100\n",
      "50000/50000 [==============================] - 82s - loss: 0.8460 - acc: 0.7096 - val_loss: 1.0324 - val_acc: 0.6823\n",
      "Epoch 72/100\n",
      "50000/50000 [==============================] - 82s - loss: 0.8416 - acc: 0.7090 - val_loss: 1.0374 - val_acc: 0.6705\n",
      "Epoch 73/100\n",
      "50000/50000 [==============================] - 84s - loss: 0.8397 - acc: 0.7098 - val_loss: 1.0557 - val_acc: 0.6831\n",
      "Epoch 74/100\n",
      "50000/50000 [==============================] - 82s - loss: 0.8410 - acc: 0.7108 - val_loss: 1.0271 - val_acc: 0.6768\n",
      "Epoch 75/100\n",
      "50000/50000 [==============================] - 84s - loss: 0.8444 - acc: 0.7093 - val_loss: 1.0393 - val_acc: 0.6689\n",
      "Epoch 76/100\n",
      "50000/50000 [==============================] - 81s - loss: 0.8388 - acc: 0.7112 - val_loss: 1.0078 - val_acc: 0.6770\n",
      "Epoch 77/100\n",
      "50000/50000 [==============================] - 80s - loss: 0.8358 - acc: 0.7128 - val_loss: 1.0064 - val_acc: 0.6789\n",
      "Epoch 78/100\n",
      "50000/50000 [==============================] - 85s - loss: 0.8334 - acc: 0.7134 - val_loss: 1.0521 - val_acc: 0.6814\n",
      "Epoch 79/100\n",
      "50000/50000 [==============================] - 85s - loss: 0.8379 - acc: 0.7108 - val_loss: 0.9914 - val_acc: 0.6860\n",
      "Epoch 80/100\n",
      "50000/50000 [==============================] - 86s - loss: 0.8361 - acc: 0.7129 - val_loss: 1.0350 - val_acc: 0.6764\n",
      "Epoch 81/100\n",
      "50000/50000 [==============================] - 87s - loss: 0.8317 - acc: 0.7153 - val_loss: 1.0752 - val_acc: 0.6633\n",
      "Epoch 82/100\n",
      "50000/50000 [==============================] - 84s - loss: 0.8291 - acc: 0.7136 - val_loss: 1.0746 - val_acc: 0.6758\n",
      "Epoch 83/100\n",
      "50000/50000 [==============================] - 88s - loss: 0.8331 - acc: 0.7149 - val_loss: 1.0210 - val_acc: 0.6776\n",
      "Epoch 84/100\n",
      "50000/50000 [==============================] - 87s - loss: 0.8306 - acc: 0.7156 - val_loss: 1.0477 - val_acc: 0.6836\n",
      "Epoch 85/100\n",
      "50000/50000 [==============================] - 89s - loss: 0.8290 - acc: 0.7153 - val_loss: 1.0861 - val_acc: 0.6732\n",
      "Epoch 86/100\n",
      "50000/50000 [==============================] - 85s - loss: 0.8314 - acc: 0.7148 - val_loss: 1.0621 - val_acc: 0.6729\n",
      "Epoch 87/100\n",
      "50000/50000 [==============================] - 87s - loss: 0.8282 - acc: 0.7171 - val_loss: 1.0294 - val_acc: 0.6798\n",
      "Epoch 88/100\n",
      "50000/50000 [==============================] - 86s - loss: 0.8263 - acc: 0.7151 - val_loss: 1.0710 - val_acc: 0.6798\n",
      "Epoch 89/100\n",
      "50000/50000 [==============================] - 87s - loss: 0.8278 - acc: 0.7142 - val_loss: 1.1061 - val_acc: 0.6873\n",
      "Epoch 90/100\n",
      "50000/50000 [==============================] - 86s - loss: 0.8233 - acc: 0.7171 - val_loss: 1.0647 - val_acc: 0.6505\n",
      "Epoch 91/100\n",
      "50000/50000 [==============================] - 87s - loss: 0.8394 - acc: 0.7129 - val_loss: 1.0626 - val_acc: 0.6664\n",
      "Epoch 92/100\n",
      "50000/50000 [==============================] - 88s - loss: 0.8299 - acc: 0.7130 - val_loss: 1.0580 - val_acc: 0.6889\n",
      "Epoch 93/100\n",
      "50000/50000 [==============================] - 87s - loss: 0.8318 - acc: 0.7180 - val_loss: 1.0854 - val_acc: 0.6379\n",
      "Epoch 94/100\n",
      "50000/50000 [==============================] - 83s - loss: 0.8294 - acc: 0.7140 - val_loss: 1.0509 - val_acc: 0.6808\n",
      "Epoch 95/100\n",
      "50000/50000 [==============================] - 81s - loss: 0.8275 - acc: 0.7189 - val_loss: 1.2365 - val_acc: 0.6849\n",
      "Epoch 96/100\n",
      "50000/50000 [==============================] - 85s - loss: 0.8253 - acc: 0.7176 - val_loss: 1.0526 - val_acc: 0.6894\n",
      "Epoch 97/100\n",
      "50000/50000 [==============================] - 83s - loss: 0.8263 - acc: 0.7175 - val_loss: 1.0871 - val_acc: 0.6876\n",
      "Epoch 98/100\n",
      "50000/50000 [==============================] - 85s - loss: 0.8291 - acc: 0.7184 - val_loss: 1.0360 - val_acc: 0.6717\n",
      "Epoch 99/100\n",
      "50000/50000 [==============================] - 84s - loss: 0.8332 - acc: 0.7155 - val_loss: 1.1084 - val_acc: 0.6793\n",
      "Epoch 100/100\n",
      "50000/50000 [==============================] - 83s - loss: 0.8311 - acc: 0.7179 - val_loss: 1.2958 - val_acc: 0.6729\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20d0d19ba90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.RMSprop(lr=0.0001, decay=1e-8)\n",
    "\n",
    "# Train the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "\n",
    "print('Not using data augmentation.')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test),\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('models/cifar10-cc-673.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del model  # deletes the existing model\n",
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "model = load_model('models/cifar10-cc-673.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9920/10000 [============================>.] - ETA: 0s[1.2958218957901, 0.67290000000000005]\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_path = 'input/cat01.jpg'\n",
    "\n",
    "img = image.load_img(img_path, target_size=(32, 32))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "\n",
    "preds = model.predict(x)\n",
    "# decode the results into a list of tuples (class, description, probability)\n",
    "# (one such list for each sample in the batch)\n",
    "#print('Predicted:', decode_predictions(preds, top=1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  6.97877200e-04   4.27309606e-05   3.86324003e-02   3.17058116e-01\n",
      "    1.21492706e-02   7.43695945e-02   5.53115845e-01   2.81954557e-03\n",
      "    9.54523784e-06   1.10504741e-03]]\n",
      "frog\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20d10bd05f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img)\n",
    "print(preds)\n",
    "print(classList[preds.argmax()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
